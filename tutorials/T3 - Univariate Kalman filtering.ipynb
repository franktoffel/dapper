{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from resources.workspace import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The (univariate) Kalman filter (KF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 3.1: \n",
    "Suppose that $\\;\\; x \\sim \\mathcal{N}(\\hat{x}, P)$ and that,  \n",
    "independently, $q \\sim \\mathcal{N}(\\hat{q}, Q)$.  \n",
    "Show that $F x + q \\;\\sim\\; \\mathcal{N}(F \\hat{x} + \\hat{q}, \\; F^2P + Q)$.  \n",
    "You may take it for granted that [the sum of two Gaussian random variables is again a Gaussian](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables#Proof_using_convolutions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Gaussian sums')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The KF estimates the state variable (truth) $x$,  \n",
    "which is assumed to \"evolve\" according to some linear \"dynamics\":\n",
    "$$ x_{k} = F_{k-1} x_{k-1} + q_{k-1} \\, , \\qquad (\\text{Dyn}) $$\n",
    "where $q_{k-1} \\sim \\mathcal{N}(0, Q)$ is known dynamic process noise,  \n",
    "and $k = 1,2,3,\\ldots$ denotes the time indices.\n",
    "\n",
    "\n",
    "The KF recursively (for increasing $k$) estimates $x_k$.\n",
    "It repeats a cycle consisting of two steps :\n",
    " \n",
    "#### The forecast step\n",
    "\"propagates\" the estimate (the pdf) of the state;\n",
    "since $x_k$ obeys (Dyn),  \n",
    "   if $\\; p(x_{k-1}) = \\mathcal{N}(x_{k-1} \\mid \\; \\phantom{F_{k-1}}\\hat{x}_{k-1}^a,\\; \\phantom{F_{k-1}^2} P_{k-1}^a) \\, ,$\n",
    "   for some $\\hat{x}_{k-1}^a, \\, P_{k-1}^a$,   \n",
    "   then  $p(x_k)     = \\mathcal{N}(x_k \\; \\; \\, \\mid \\;  \\underbrace{F_{k-1}\n",
    "   \\hat{x}_{k-1}^a}_{{\\hat{x}_{k}}^f},\\; \\underbrace{F_{k-1}^2 P_{k-1}^a + Q}_{{P_k}^f}) \\, .$  \n",
    "The KF forecast step amounts to the computation of the moments with superscript $f$. \n",
    "\n",
    " \n",
    "#### The analysis step\n",
    "\"updates\" the prior (forecast), $\\mathcal{N}(x_k \\mid \\; \\hat{x}_k^f,\\; P_k^f)$,  \n",
    "based on the likelihood, $\\quad\\;\\;\\;\\, \\mathcal{N}(y_k \\mid \\, x_k, \\; R)$,  \n",
    "into the posterior (analysis), $\\; \\; \\, \\mathcal{N}(x_k \\mid \\; \\hat{x}_{k}^a, \\, P_{k}^a)$.  \n",
    "The update formulae was derived as the Gaussian-Gaussian Bayes' rule in [the previous tutorial](T2%20-%20Bayesian%20inference.ipynb#Gaussian-Gaussian-Bayes). It can be applied with the symbolic translations in the below table.\n",
    "\n",
    "|        |                 |         |                 |         |\n",
    "| ---    | ---             | ---     | ---             | ---     |\n",
    "| Tut 2: | $b$             | $B$     | $\\hat{x}$       | $P$     |\n",
    "| Tut 3: | $\\hat{x}_{k}^f$ | $P_k^f$ | $\\hat{x}_{k}^a$ | $P_k^a$ |\n",
    "\n",
    "\n",
    "The KF analysis step amounts to the computation of the moments with superscript $a$.\n",
    "This completes the cycle, which can then restart with the forecast from $k$ to $k+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A straight-line example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many mathematical methods are tagged as \"least squares\". They typically have one thing in common: some sum of squared terms is being minimized. Both (least-squares) linear regression and Kalman filtering (KF) may be derived from \"least squares\".\n",
    "Do they yield the same estimate (when applied to the linear regression problem)? That's what we'll investigate...\n",
    "\n",
    "Consider the straight line\n",
    "$$x_k = a k \\, ,     \\qquad \\qquad (1) $$\n",
    "and suppose we have observations ($y$) of the line, but corrupted by noise ($r$):\n",
    "\\begin{align*}\n",
    "y_k &= x_k + r_k \\, , \\;\\; \\qquad (2)\n",
    "\\end{align*}\n",
    "where $r_k \\sim \\mathcal{N}(0, R)$ for some $R>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below sets up an experiment based on eqns. (1) and (2).  \n",
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 0.4  # Slope (xx[k] = a*k) paramterer. Unknown to be estimated.\n",
    "K = 10   # Length of experiment (final time index)\n",
    "R = 1    # Observation noise strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following simulates a series of the truth ($x$) and observations ($y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Recall the naming convention: xx and yy hold time series of x and y.\n",
    "xx = np.zeros(K+1) # truth states\n",
    "yy = np.zeros(K+1) # obs\n",
    "\n",
    "for k in 1+arange(K):\n",
    "    xx[k] = a*k\n",
    "    yy[k] = xx[k] + sqrt(R)*randn()\n",
    "\n",
    "# The obs at k==0 should not be used (since we know xx[0]==0, it is worthless).\n",
    "yy[0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esitmation by linear regression\n",
    "(Least-squares) linear regression minimizes\n",
    "$$J_K(\\hat{a}) = \\sum_{k=1}^K (y_k - \\hat{a} k)^2 \\, ,  \\qquad (4)$$\n",
    "yielding the estimator\n",
    "$$\\hat{a} = \\frac{\\sum_{k=1}^K {k} y_{k}}{\\sum_{k=1}^K {k}^2} \\, . \\qquad \\qquad (6)$$\n",
    "\n",
    "**Exc 3.2:** Derive (6) from (4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('LinReg deriv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.4:** Program the linear regresson estimator (6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lin_reg(k):\n",
    "    \"Liner regression estimator based on observations y_1, ..., y_k.\"\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('LinReg_k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@interact(k=IntSlider(min=1, max=K))\n",
    "def plot_experiment(k):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    kk = arange(k+1)\n",
    "    plt.plot(kk,xx[kk]        ,'k' ,label='true state ($x$)')\n",
    "    plt.plot(kk,yy[kk]        ,'k*',label='noisy obs ($y$)')\n",
    "    plt.plot(kk,kk*lin_reg(k) ,'r' ,label='Linear regress.')\n",
    "\n",
    "    ### Uncomment this block AFTER doing the Exc 3.8 ###\n",
    "    # pw_xxf, pw_xxa = weave_fa(xxf,xxa)\n",
    "    # pw_kkf, pw_kka = weave_fa(arange(K+1))    \n",
    "    # plt.plot(pw_kkf[:3*k],pw_xxf[:3*k] ,'c'  ,label='KF forecasts')\n",
    "    # plt.plot(pw_kka[:3*k],pw_xxa[:3*k] ,'b'  ,label='KF analyses')\n",
    "    # #plt.plot(kk,kk*xxa[k]/k            ,'g--',label='KF extrapolated')\n",
    "\n",
    "    plt.xlim([0,1.01*K])\n",
    "    plt.ylim([-1,1.2*a*K])\n",
    "    plt.xlabel('time index (k)')\n",
    "    plt.ylabel('$x$, $y$, and $\\hat{x}$')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Estimation by the KF\n",
    "In the following we tacke the same problem, but using the KF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observations equation (2)\n",
    "yields the likelihood $p(y_k|x_k) = \\mathcal{N}(y_k \\mid x_k,R)$.  \n",
    "Hopefully this is intuitive; otherwise, a more detailed derivation will be given in tutorial 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.6:** For the (univariate) KF,\n",
    "we need to reformulate the problem of estimating the parameter $a$ \n",
    "as the problem of estimating $x_k$ (yielding $\\hat{a}_k = \\hat{x}_k / k$).\n",
    "Derive and implement the \"forecast model\" $F_k$ (a function of $k>0$ only) such that\n",
    "the recursion $x_{k+1} = F_k x_k$ is equivalent to eqn (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F(k):\n",
    "    return ### INSERT ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Sequential 2 Recusive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.7:** Explain why the KF is applicable to a larger class of problems than linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('LinReg âŠ‚ KF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So although the KF (and its implementation below) may seem like \"overkill\" for this problem,\n",
    "this \"heavy machinery\" can do a lot more, and will pay off later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.8:** Implement the KF (described at the top of this notebook) in the below code block to estimate $x_k$ for $k=1,\\ldots, K$.  \n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<b>NB:</b> for this example, do not use the \"Kalman gain\" form of the analysis update.\n",
    "This problem involves the peculiar, unrealistic situation of infinities\n",
    "(related to \"improper priors\") at `k==1`, yielding platform-dependent behaviour.\n",
    "These peculariaties are of mainly of academic interest, and is not our focus here.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q = 0 # Dynamical model noise strength\n",
    "\n",
    "# Allocation\n",
    "xxa = np.zeros(K+1) # mean estimates -- analysis values (a)\n",
    "xxf = np.zeros(K+1) # mean estimates -- forecast values (f)\n",
    "PPa = np.zeros(K+1) # var  estimates -- analysis values (a)\n",
    "PPf = np.zeros(K+1) # var  estimates -- forecast values (f)\n",
    "\n",
    "def KF(k):\n",
    "    \"A cycle of the Kalman filter\"\n",
    "    # Forecast\n",
    "    if k==1:\n",
    "        PPf[k] = np.inf # The \"initial\" prior uncertainty is infinite...\n",
    "        xxf[k] = 0      # ... thus the corresponding mean is inconsequential.\n",
    "    else:\n",
    "        PPf[k] = ### INSERT ANSWER HERE ###\n",
    "        xxf[k] = ### INSERT ANSWER HERE ###\n",
    "    # Analysis\n",
    "    PPa[k] = ### INSERT ANSWER HERE ###\n",
    "    xxa[k] = ### INSERT ANSWER HERE ###\n",
    "\n",
    "# Run estimations/computations\n",
    "for k in 1+arange(K):\n",
    "    KF(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('KF_k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.10:** Go back to the animation above and uncomment the block that plots the KF estimates.  \n",
    "Visually: what is the relationship between the estimates provided by the KF and by linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('LinReg compare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Exercises marked with an asterisk (*) are optional.</em>\n",
    "\n",
    "**Exc 3.12*:** This excercise proves (on paper) the result of the previous excercise.  \n",
    "Note that the KF forecast step (here with $Q=0$) can be inserted in the analysis step, forming a single couple of recursions (and making the 'a' superscript unnecessary), which are recursive:\n",
    "\n",
    " * $\\hat{x}_k = P_k \\big(y_k/R \\;+\\; F_{k-1} \\hat{x}_{k-1} / [F_{k-1}^2 P_{k-1}] \\big) \\qquad (11)$  \n",
    " * $P_k = 1/\\big(1/R \\;+\\; 1/[F_{k-1}^2 P_{k-1}]\\big) \\qquad \\qquad \\quad (12)$\n",
    "\n",
    "Now:\n",
    "\n",
    "* (a). First show that $P_K = R\\frac{K^2}{\\sum_{k=1}^K k^2} \\, . \\;\\;\\; \\qquad \\quad (13)$\n",
    "* (b). Then show that $\\hat{x}_K = K\\frac{\\sum_{k=1}^K k y_k}{\\sum_{k=1}^K k^2} = K \\hat{a}_K$, where $\\hat{a}_K$ is given by eqn (11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#show_answer('x_KF == x_LinReg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 3.14:\n",
    "Let $x_{k+1} = F x_k$, *for a generic $F>1$ (note: $Q=0$)*. What does the sequence of $P_k$ converge to?  \n",
    "You must start from eqn (12), because eqn (13) is for the straight-line example only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Asymptotic P when F>1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 3.15:\n",
    "Redo Exc 3.14, but assuming  \n",
    " * (a) $F = 1$.\n",
    " * (b) $F < 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Asymptotic P when F=1')\n",
    "#show_answer('Asymptotic P when F<1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, if $F>1$, the KF does not converge to 0 error. This is because, even though you keep gaining more information, this is balanced by the growth in uncertainty by the forecast. On the other hand, if $F \\leq 1$ then the error converges to zero.\n",
    "\n",
    "In general, however, $F$ depends on $k$, as will the other system matrices ($Q, R$).\n",
    "Then, there is no limit value that the state distribution (and its parameters) converges to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.18*:** Set $Q$ to 1 or more. Re-compute the KF estimates. By interpreting the meaning of $Q$, explain why the KF estimate is now closer to the obs (always at the latest time instance) than the linear regression estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.20*:** Now change $R$ (but don't re-run the simulation of the truth and obs). The KF estimates should not change (in this particular example). Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "The KF consists of two steps:\n",
    " * Forecast\n",
    " * Analysis\n",
    " \n",
    "In each step, the mean ($\\hat{x}$) and variance ($P$) must be updated. \n",
    "\n",
    "As an example, we saw that the linear regression estimate is reproduced by the KF, although it is a bit tricky to initialize the KF with infinite uncertainty. However, the KF (i.e. state estimation) is much more general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [Time series analysis](T4 - Time series analysis.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
