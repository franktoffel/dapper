{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from resources.workspace import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Gaussian (i.e. Normal) distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the Gaussian random variable $x \\sim \\mathcal{N}(b,B)$; \n",
    " - the first parameter $(b)$ indicates its mean;\n",
    " - the second parameter $(B)$ indicates its variance.\n",
    " \n",
    "Sometimes it is more convenient to write\n",
    "$$ p(x) = \\mathcal{N}(x \\mid b,B) \\, , $$\n",
    "for its probability density function (**pdf**), which is given by\n",
    "$$ \\mathcal{N}(x \\mid b,B) = (2 \\pi B)^{-1/2} e^{-(x-b)^2/2 B} \\, .  \\qquad \\qquad (G1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.2:** Code it up (complete the code below)! Hints:\n",
    "* Note that `**` is the power operator in Python.\n",
    "* As in Matlab, $e^x$ is available as `exp(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Univariate (scalar), Gaussian pdf\n",
    "def pdf_G1(x,b,B):\n",
    "    # pdf_values = ### INSERT ANSWER HERE ###\n",
    "    return pdf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('pdf_G1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's plot the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b  = 0                  # mean     of distribution\n",
    "B  = 25                 # variance of distribution\n",
    "\n",
    "# Plotting\n",
    "N  = 201                # num of grid points\n",
    "xx = linspace(-20,20,N) # grid\n",
    "dx = xx[1]-xx[0]        # grid spacing\n",
    "pp = pdf_G1(xx,b,B)     # pdf values\n",
    "plt.subplot(211)        # allocate plot panel\n",
    "plt.plot(xx,pp);        # plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could for example be the pdf of a stochastic noise variable. It could also describe our *quantitative belief* and uncertainty about a parameter (or state), which we model as randomness in Bayesian statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.4:** From looking at the figure:\n",
    " * How does the pdf curve change when `b` changes?  \n",
    "   (alter the above code, and re-run the cell)\n",
    " * How does the pdf curve change when you increase `B`?  \n",
    " \n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<b>NB:</b> Restore `B=25` and re-run (this is a convienient value for examples)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.6:** Recall $p(x) = \\mathcal{N}(x \\mid b,B)$ from eqn (G1).  \n",
    "The following are helpful points to remember how it looks. Use pen, paper, and calculus.  \n",
    " * (i) Where is the location of the mode (maximum) of the distribution?\n",
    "I.e. find $x$ such that $\\frac{d p}{d x}(x) = 0$.  \n",
    "Hint: it's usually easier to analyse $\\log p(x)$ rather than $p(x)$ itself.\n",
    " * (ii) Where is the inflection point? I.e. where $\\frac{d^2 p}{d x^2}(x) = 0$.\n",
    " * (iii) Some forms of \"sensitivy analysis\" (a basic form of uncertainty quantification) consist in evaluating $\\frac{d^2 p}{d x^2}(x)$ at the mode. Explain this by reference to the Gaussian shape.\n",
    "Hint: calculate and interpret $\\frac{d^2 p}{d x^2}(b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The multivariate (i.e. vector) case\n",
    "Here's the pdf of the *multivariate* Gaussian:\n",
    "\\begin{align*}\n",
    "\\mathcal{N}(x \\mid  b,B) &= |2 \\pi B|^{-1/2} \\, \\exp\\Big(-\\frac{1}{2}\\|x-b\\|^2_B\\Big) \\, , \\qquad \\qquad (GM) \\\\\\\n",
    "\\end{align*}\n",
    "where $|.|$ represents the determinant, and $\\|.\\|_W$ represents the norm with weighting: $\\|x\\|^2_W = x^T W^{-1} x$.  \n",
    "In the multivariate case, $B$ is called the *covariance* (matrix).\n",
    "\n",
    "The following implements this pdf. Take a moment to digest the code. But don't worry yet if you don't understand all of the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import det, inv\n",
    "\n",
    "def weighted_norm22(xx,W):\n",
    "    \"Computes the norm of each row vector of xx, as weighted by W.\"\n",
    "    ww = np.sum((xx @ inv(W)) * xx, axis=1)    # \"vectorized\" (fast) version\n",
    "    #ww = array([ x @ inv(W) @ x for x in xx]) # \"loop\" version.\n",
    "    return ww\n",
    "\n",
    "def pdf_GM(xx,b,B):\n",
    "    \"pdf -- Gaussian, Multivariate: N(x|b,B)\"\n",
    "    return 1/sqrt(det(2*pi*B))*exp(-0.5*weighted_norm22(xx-b,B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips:\n",
    " * `@` produces matrix multiplication (`*` in `Matlab`);\n",
    " * `*` produces array multiplication (`.*` in `Matlab`);\n",
    " * `axis=1` makes `np.sum()` work along the horizontal dimension of a 2D-array (matrix);\n",
    " * `axis=0` makes `np.sum()` work along the vertical dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code plots the pdf as contour (iso-density) curves.  \n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<b>NB:</b> The plot appears in the above figure.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grid\n",
    "def flatten(xxyy): return array([xi.ravel() for xi in xxyy]).T\n",
    "def  square(xyxy): return xyxy.reshape(int(sqrt(len(xyxy))),-1)\n",
    "\n",
    "grid = flatten(np.meshgrid(xx,xx))\n",
    "\n",
    "# Covariance specification\n",
    "corr = 0.7\n",
    "var1 = 1\n",
    "var2 = 1\n",
    "cov12 = sqrt(var1*var2)*corr\n",
    "Cov = B * array([[var1  , cov12],\n",
    "                 [cov12 , var2]])\n",
    "# Eval\n",
    "pp = pdf_GM(grid, 0, Cov)\n",
    "pp = square(pp)\n",
    "\n",
    "# Plot\n",
    "plt.subplot(212).clear()\n",
    "plt.contour(xx,xx,pp)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.8:** How do the contours look? Try to understand why. Cases:\n",
    " * (a) correlation=0.    \n",
    " * (b) correlation=0.99.\n",
    " * (c) correlation=0.5. (Note that we've used `plt.axis('equal')`).\n",
    " * (d) correlation=0.5, but with non-equal variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.9:** Go play the [correlation game](http://guessthecorrelation.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes' rule\n",
    "Bayes' rule is how we do inference.  \n",
    "It defines how we should merge our prior (quantitative belief) about $x$  \n",
    "when given an observation $y$ somehow related to $x$.\n",
    "\n",
    "\n",
    "For continuous random variables, $x$ and $y$, it reads:\n",
    "\n",
    "$$ p(x|y) = \\frac{p(x) \\, p(y|x)}{p(y)} \\, , \\qquad \\qquad (2)$$\n",
    "\n",
    "or, in words:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{\"posterior\" (pdf of $x$ given $y$)}\n",
    "\\; = \\;\n",
    "\\frac{\\text{\"prior\" (pdf of $x$)}\n",
    "\\; \\times \\;\n",
    "\\text{\"likelihood\" (pdf of $y$ given $x$)}}\n",
    "{\\text{\"normalization\" (pdf of $y$)}} \\, .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_example('BR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.10:** Derive Bayes' rule from the definition of [conditional pdf's](https://en.wikipedia.org/wiki/Conditional_probability#Kolmogorov_definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('BR derivation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Exercises marked with an asterisk (*) are optional.</em>\n",
    "\n",
    "**Exc 2.11*:** Laplace also independently (and more clearly) developed Bayes' rule, published in 1774. Some time therafter, what we now call \"statistical inference\" came to be known as the reasoning of \"inverse probability\". Nowadays, \"inverse problems\" are often given a statistical interpretation. Considering this context, why do you think we use $x$ for the \"unknown\", and $y$ for the known/given/fixed data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('inverse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers generally work with discrete, numerical representations of mathematical entities.\n",
    "Numerically, pdfs may be represented by their `values` on a grid, such as `xx` from above. Bayes' rule (2) then consists of *grid-point-wise* multiplication, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bayes_rule(prior_values,lklhd_values,dx):\n",
    "    \"Numerical (pointwise) implementation of Bayes' rule.\"\n",
    "    pp = prior_values * lklhd_values   # pointwise multiplication\n",
    "    posterior_values = pp/(sum(pp)*dx) # normalization\n",
    "    return posterior_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below show's Bayes' rule in action.  \n",
    "Again, remember that the only thing it's doing is multiplying the prior and likelihood at each gridpoint.  \n",
    "Move the sliders with the arrow keys to animate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Fix the prior's parameters\n",
    "b = 0 # mean\n",
    "B = 1 # variance\n",
    "\n",
    "@interact(y=(-10,10,1),R=(0.01,20,0.2))\n",
    "def animate_Bayes(y=4.0,R=1):\n",
    "    prior_vals = pdf_G1(xx,b,B)\n",
    "    lklhd_vals = pdf_G1(y,xx,R)\n",
    "    \n",
    "    postr_vals = Bayes_rule(prior_vals, lklhd_vals, xx[1]-xx[0])\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(xx,prior_vals, label='prior $\\mathcal{N}(x|b,B)$')\n",
    "    plt.plot(xx,lklhd_vals, label='likelihood $\\mathcal{N}(y|x,R)$')\n",
    "    plt.plot(xx,postr_vals, label='posterior - pointwise')\n",
    "    \n",
    "    ### Uncomment this block AFTER doing the Exc 2.24 ###\n",
    "    # mu, P = Bayes_rule_G1(b,B,y,R)\n",
    "    # postr_vals2 = pdf_G1(xx,mu,P)\n",
    "    # plt.plot(xx,postr_vals2,'--',label='posterior - parametric\\n $\\mathcal{N}(x|mu,P)$')\n",
    "    \n",
    "    plt.ylim(ymax=0.6)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.12:** This excercise serves to make you acquainted with how Bayes' rule blends information.  \n",
    "Move the sliders to see what happens, and answer the following:\n",
    " * What happens to the posterior when $R \\rightarrow \\infty$ ?\n",
    " * What happens to the posterior when $R \\rightarrow 0$ ?\n",
    " * Move around $y$. What is the posterior's location (mean/mode) when $R = B$ ?\n",
    " * Does the posterior scale (width) depend on $y$?\n",
    " * Consider the shape (ignoring location & scale) of the posterior. Does it depend on $R$ or $y$?\n",
    " * Can you see a shortcut to computing this posterior rather than having to do the pointwise multiplication?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Posterior behaviour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.14:** Show that the normalization in `Bayes_rule()` amounts to the same as dividing by $p(y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('BR normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, since $p(y)$ is implicitly known (c.f. Exc 2.14),\n",
    "we often don't bother to write it down, simplifying Bayes' rule (2) to\n",
    "$$ p(x|y) \\propto p(x) \\, p(y|x) \\, .  \\qquad \\qquad (3) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.15*:** \n",
    "* (a) Implement a \"uniform\" (or \"flat\" or \"box\") distribution pdf and call it `pdf_U1(x,mu,P)`. These <a href=\"https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)#Moments\">formulae</a> for its mean/variance will be useful.  \n",
    "In the above animations, replace `pdf_G1` with your new `pdf_U1` (both for the prior and likelihood). Ensure that everything is working correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('pdf_U1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (b) \n",
    " - Why (in the figure) are the walls of the pdf (ever so slightly) inclined?\n",
    " - What happens when you move the prior and likelihood too far apart? Is the fault of the implementation, the math, or the problem statement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('BR U1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (c)*:\n",
    " - Re-do Exc 2.12, now with `pdf_U1`.\n",
    "* (d)*:\n",
    " - Now test a Gaussian prior with a uniform likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<b>NB:</b> At the end of this excercise, restore `pdf_G1` (both the prior and likelihood) in the above animation (for later use). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gaussian-Gaussian Bayes\n",
    "\n",
    "The above animation shows Bayes' rule in 1 dimension. Previously, we saw how a Gaussian looks in 2 dimensions. Can you imagine how Bayes' rule looks in 2 dimensions? In higher dimensions, these things get difficult to imagine, let alone visualize.\n",
    "\n",
    "Similarly, the size of the calculations required for Bayes' rule poses a difficulty. Indeed, the following exercise shows that (pointwise) multiplication for all grid points becomes preposterious in high dimensions.\n",
    "\n",
    "**Exc 2.16:**\n",
    " * (a) How many point-multiplications are needed on a grid with $N$ points in $M$ dimensions? (Imagine an $M$-dimensional cube where each side has a grid with $N$ points on it)\n",
    " * (b) Suppose we model 15 physical quanitites, on each grid point, on a discretized surface model of Earth. Assume the resolution is $1^\\circ$ for latitude (110km), $1^\\circ$ for longitude. How many variables are there in total? This is the dimensionality ($M$) of the problem.\n",
    " * (c) Suppose each variable is has a pdf represented with a grid using only $N=10$ points. How many multiplications are necessary to calculate Bayes rule (jointly) for all variables on our Earth model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Dimensionality a')\n",
    "#show_answer('Dimensionality b')\n",
    "#show_answer('Dimensionality c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In response to this computational difficulty, we try to be smart and do something more analytical (\"pen-and-paper\"): we only compute the parameters (mean and (co)variance) of the posterior pdf.\n",
    "\n",
    "This is doable and quite simple in the Gaussian-Gaussian case:  \n",
    "With a prior $p(x) = \\mathcal{N}(x \\mid b,B)$ and a likelihood $p(y|x) = \\mathcal{N}(y \\mid x,R)$,\n",
    "the posterior is\n",
    "\\begin{align*}\n",
    "p(x|y)\n",
    "&= \\mathcal{N}(x \\mid \\hat{x},P) \\qquad \\qquad (4) \n",
    "\\, ,\n",
    "\\end{align*}\n",
    "where, in the univarite (1-dimensional) case:\n",
    "\\begin{align*}\n",
    "    P &= 1/(1/B + 1/R) \\, , \\qquad \\qquad (5) \\\\\\\n",
    "  \\hat{x} &= P(b/B + y/R) \\, .  \\,\\qquad \\qquad (6) \n",
    "\\end{align*}\n",
    "\n",
    "The multivariate case is discussed in a later tutorial; for now, try to tackle exc 2.18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc  2.18 'Gaussian Bayes':\n",
    "Derive the above expressions for $P$ and $\\hat{x}$\n",
    "from Bayes' rule (3) and the expression for a Gaussian pdf (G1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('BR Gauss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.20:** Do some light algebra to show that eqns (5) and (6) can be rewritten as\n",
    "\\begin{align*}\n",
    "    P &= (1-K)B \\, ,  \\qquad \\qquad (8) \\\\\\\n",
    "  \\hat{x} &= b + K (y-b)  \\qquad \\quad (9) \\, ,\n",
    "\\end{align*}\n",
    "where $K = B/(B+R)$, which is called the \"Kalman gain\". This form of the update becomes highly interesting in the multivariate case (later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.22*:** Consider the formula for $K$ and its role in the previous couple of equations. Why do you think $K$ is called a \"gain\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('KG intuition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.24:** Implement a Gaussian-Gaussian Bayes' rule (eqns 5 and 6, or eqns 8 and 9) by completing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bayes_rule_G1(b,B,y,R):\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    return mu,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('BR Gauss code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.26:** Go back to the above animation code, and uncomment the block that uses `Bayes_rule_G1()`. Re-run.  \n",
    "Make sure its curve coincides with that which uses pointwise multiplication (i.e. `Bayes_rule()`).\n",
    "This is the main secret of the \"Kalman filter\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.28:** Questions related to the above animation:\n",
    " * Does the width (i.e. scale) for the posterior depend on the location $y$ of the likelihood?\n",
    " * Note that the width (i.e. scale) for the posterior always smaller than that of prior and likelihood.\n",
    "   * What does this mean [information-wise](https://en.wikipedia.org/wiki/Differential_entropy#Differential_entropies_for_various_distributions)?\n",
    "   * Is this ordering also always true for non-Gaussian distributions?\n",
    " * Imagine that you're pretty sure about something, but then you get a wildly different indication (observation).  \n",
    "   What is your posterior uncertainty? Has it decreased?  \n",
    "   So, are you a Gaussian thinker?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Posterior cov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.30*:** Why are we so fond of the Gaussian assumption?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Why Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [Univariate (scalar) Kalman filtering](T3 - Univariate Kalman filtering.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
