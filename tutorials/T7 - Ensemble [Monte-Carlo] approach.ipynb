{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from resources.workspace import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ensemble (Monte-Carlo) approach\n",
    "is an approximate method for doing Bayesian inference. Instead of computing the full posterior distributions, we instead try to generate ensembles from them.\n",
    "\n",
    "An ensemble is an *iid* sample. I.e. a set of \"members\" (\"particles\", \"realizations\", or \"sample points\") that have been drawn (\"sampled\") independently from the same distribution. With the EnKF, these assumptions are generally tenous, but pragmatic.\n",
    "\n",
    "Ensembles can be used to characterize uncertainty: either by reconstructing (estimating) the distribution from which it is assumed drawn, or by computing various *statistics* such as the mean, median, variance, covariance, skewness, confidence intervals, etc (any function of the ensemble can be seen as a \"statistic\"). This is illustrated by the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "b   = 0\n",
    "B   = 25    \n",
    "B12 = sqrt(B)\n",
    "\n",
    "def true_pdf(x):\n",
    "    return ss.norm.pdf(x,b,sqrt(B))\n",
    "\n",
    "# Plot true pdf\n",
    "xx = 3*linspace(-B12,B12,201)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xx,true_pdf(xx),label=\"True\");\n",
    "\n",
    "# Sample and plot ensemble\n",
    "M = 1   # length of state vector\n",
    "N = 100 # ensemble size\n",
    "E = b + B12*randn((N,M))\n",
    "ax.plot(E, zeros(N), '|k', alpha=0.3, ms=100)\n",
    "\n",
    "# Plot histogram\n",
    "nbins = max(10,N//30)\n",
    "heights, bins, _ = ax.hist(E,normed=1,bins=nbins,label=\"Histogram estimate\")\n",
    "\n",
    "# Plot parametric estimate\n",
    "x_bar = np.mean(E)\n",
    "B_bar = np.var(E)\n",
    "ax.plot(xx,ss.norm.pdf(xx,x_bar,sqrt(B_bar)),label=\"Parametric estimate\")\n",
    "\n",
    "ax.legend();\n",
    "\n",
    "# Uncomment AFTER Exc 4:\n",
    "# dx = bins[1]-bins[0]\n",
    "# c = 0.5/sqrt(2*pi*B)\n",
    "# for height, x in zip(heights,bins):\n",
    "#     ax.add_patch(mpl.patches.Rectangle((x,0),dx,c*height/true_pdf(x+dx/2),alpha=0.3))\n",
    "# Also set\n",
    "#  * N = 10**4\n",
    "#  * nbins = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2:** Which approximation to the true pdf looks better: Histogram or the parametric?   \n",
    "Does one approximation actually start with more information? The EnKF takes advantage of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 4*:** Suppose the histogram bars get normalized (divided) by the value of the pdf at their location.  \n",
    "How do you expect the resulting histogram to look?  \n",
    "Test your answer by uncommenting the block in the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 5*:\n",
    "Use the method of `gaussian_kde` from `scipy.stats` to make a \"continuous histogram\" and plot it above.\n",
    "`gaussian_kde`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer(\"KDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 6 (Multivariate Gaussian sampling):**\n",
    "Suppose $\\mathbf{z}$ is a standard Gaussian,\n",
    "i.e. $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z} \\mid 0,\\mathbf{I}_M)$,\n",
    "where $\\mathbf{I}_M$ is the $M$-dimensional identity matrix.  \n",
    "Let $\\mathbf{x} = \\mathbf{L}\\mathbf{z} + \\mathbf{b}$. \n",
    "Recall [Exc 3.1](T3%20-%20Univariate%20Kalman%20filtering.ipynb#Exc-3.1:),\n",
    "which yields $p(\\mathbf{x}) = \\mathcal{N}(\\mathbf{x}Â \\mid \\mathbf{b}, \\mathbf{L}^{}\\mathbf{L}^T)$.\n",
    "    \n",
    " * (a). $\\mathbf{z}$ can be sampled using `randn((M,1))`. How (where) is `randn` defined?\n",
    " * (b). Consider the above definition of $\\mathbf{x}$ and the code below.\n",
    " Complete it so as to generate a random realization of $\\mathbf{x}$.  \n",
    " Hint: matrix-vector multiplication can be done using the symbol `@`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M   = 3 # ndim\n",
    "b   = 10*ones(M)\n",
    "B   = diag(1+arange(M))\n",
    "L   = np.linalg.cholesky(B) # B12\n",
    "print(\"True mean and cov:\")\n",
    "print(b)\n",
    "print(B)\n",
    "\n",
    "### INSERT ANSWER (b) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Gaussian sampling a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Gaussian sampling b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * (c). Now sample $N = 100$ realizations of $\\mathbf{x}$\n",
    " and collect them in an $M$-by-$N$ \"ensemble matrix\" $\\mathbf{E}$.  \n",
    " Avoid `for` loops (the main thing to figure out is:\n",
    " how to add a (mean) vector to a matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N  = 100 # ensemble size\n",
    "\n",
    "E = ### INSERT ANSWER (c) ###\n",
    "\n",
    "# Use the code below to assess whether you got it right\n",
    "x_bar = np.mean(E,axis=1)\n",
    "B_bar = np.cov(E)\n",
    "print(\"Estimated mean and cov:\")\n",
    "with printoptions(precision=1):\n",
    "    print(x_bar)\n",
    "    print(B_bar)\n",
    "plt.matshow(B_bar,cmap=\"Blues\"); plt.grid('off'); plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Gaussian sampling c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 8*:** How erroneous are the ensemble estimates on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Average sampling error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 10:** Given the previous ensemble matrix $\\mathbf{E}$, compute its sample mean $\\overline{\\mathbf{x}}$ and covariance matrix, $\\overline{\\mathbf{B}}$:\n",
    "$$ \\overline{\\mathbf{x}} = \\frac{1}{N}   \\sum_{n=1}^N \\mathbf{x}_n \\\\\n",
    "   \\overline{\\mathbf{B}} = \\frac{1}{N-1} \\sum_{n=1}^N (\\mathbf{x}_n - \\overline{\\mathbf{x}}) (\\mathbf{x}_n - \\overline{\\mathbf{x}})^T  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Don't use numpy's mean, cov\n",
    "def estimate_mean_and_cov(E):\n",
    "    M, N = E.shape\n",
    "    \n",
    "    ### INSERT ANSWER ###\n",
    "    \n",
    "    return x_bar, B_bar\n",
    "\n",
    "x_bar, B_bar = estimate_mean_and_cov(E)\n",
    "print(x_bar)\n",
    "print(B_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('ensemble moments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 12:** Why is the normalization by $(N-1)$ for the covariance computation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Why (N-1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 14:** Like Matlab, Python (numpy) is quicker if you \"vectorize\" loops. This is emminently possible with computations of ensemble moments. \n",
    " * (a). Let $\\mathbf{X} = \\begin{bmatrix}\n",
    "\t\t\\mathbf{x}_1 -\\mathbf{\\bar{x}}, & \\ldots & \\mathbf{x}_n -\\mathbf{\\bar{x}}, & \\ldots & \\mathbf{x}_N -\\mathbf{\\bar{x}}\n",
    "\t\\end{bmatrix} \\, .\n",
    "\t$\n",
    "Show that $\\overline{\\mathbf{B}} = \\mathbf{X} \\mathbf{X}^T /(N-1)$.\n",
    " * (b). Code up this formula for $\\overline{\\mathbf{B}}$ and insert it in `estimate_mean_and_cov(E)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('ensemble moments vectorized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 16:** Implement the cross-covariance estimator $\\overline{Cov(\\mathbf{x},\\mathbf{y})} = \\frac{1}{N-1} \\sum_{n=1}^N (\\mathbf{x}_n - \\overline{\\mathbf{x}}) (\\mathbf{y}_n - \\overline{\\mathbf{y}})^T $.  \n",
    "If you can, use a vectorized form similarly to Exc 14a. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def estimate_cross_cov(Ex,Ey):\n",
    "    ### INSERT ANSWER ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('estimate cross')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 18 (error notions)*:**\n",
    " * (a). What's the difference between error residual?\n",
    " * (b). What's the difference between error and bias?\n",
    " * (c). Show `MSE = RMSE^2 = Bias^2 + Var`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [Writing your own EnKF](T8 - Writing your own EnKF.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
